# -*- coding: utf-8 -*-
"""Movie Genre Prediction NLP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i0KTGcuQNvEsJvZzqIWKDlGAZ1bT4b3A

# **Movie Genre Prediction NLP Model**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/imdb_movies.csv.zip')
df.head()

df.drop(columns=['names','date_x','score','orig_title','status','orig_lang','budget_x','revenue','country'], inplace=True)

df.drop(columns=['crew'],inplace=True)

df.head()

df.shape

df["genre"].value_counts()

# First, convert all genres to strings (safe handling for NaN or floats)
df['genre'] = df['genre'].astype(str)

# Now filter: keep only rows where genre does NOT contain a comma
df_single_genre = df[~df['genre'].str.contains(',')]

df=df_single_genre

print(df['genre'].value_counts())

# Step 1: Count genres
genre_counts = df['genre'].value_counts()

# Step 2: Filter genres with more than 86 occurrences
common_genres = genre_counts[genre_counts > 86].index

# Step 3: Keep only rows with those common genres
df_filtered = df[df['genre'].isin(common_genres)]

print(df_filtered['genre'].value_counts())

df=df_filtered

df['genre'].value_counts()

df.head()

"""# **Balancing (RandomOverSampling)**"""

from imblearn.over_sampling import RandomOverSampler

# Assuming df is your dataframe with the "category" column
X = df.drop("genre", axis=1)  # features
y = df["genre"]  # target

# Initialize the RandomOverSampler
ros = RandomOverSampler(random_state=42)

# Apply the oversampling
X_resampled, y_resampled = ros.fit_resample(X, y)

# Combine the resampled features and target back into a DataFrame
df_resampled = X_resampled.copy()
df_resampled["genre"] = y_resampled

# Check the new distribution
print(df_resampled["genre"].value_counts())

df = df_resampled

df["genre"].value_counts()

sns.countplot(data=df, x="genre", palette="Set1", orient='h',legend=False)
plt.show()

a = df['genre'].unique()
plt.pie(df["genre"].value_counts(),labels=a,autopct="%.2f%%")
plt.show()

df["genre"].unique()

df.head()

"""# **Text Preprocessing**"""

import nltk
# Download the 'punkt_tab' data package
nltk.download('punkt_tab')
import string
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords

from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

# Lower case
# Tokenize
# Removing Special characters
# Removing stop words and punctuation
# Lemmatization

def transform(text):
    text=text.lower()
    text=nltk.word_tokenize(text)

    y=[]
    for i in text:
         if i.isalnum(): #isalnum mean alphanumeric or numeric
            y.append(i)

    text=y[:]
    y.clear()
    for i in text:
        if i not in string.punctuation and i not in stopwords.words('english'):
            y.append(i)

    text = y[:]
    y.clear()
    for i in text:
        y.append(lemmatizer.lemmatize(i))

    return " ".join(y)

df["Transformed_Text"]=df["overview"].apply(transform)

df.head()

"""# **Label Encoding and TFIDF**"""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['genre_encoded'] = le.fit_transform(df['genre'])

df['genre_encoded'].unique()

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()

x = tfidf.fit_transform(df["Transformed_Text"]).toarray()

df.head()

y=df["genre_encoded"].values
y

"""# **Test Train Split**"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=47)

"""# **Model Building**"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

model = MultinomialNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred)*100)

model.score(X_train,y_train)*100

df.head()

"""# **Streamlit App**"""

import joblib

# Save LabelEncoder
joblib.dump(le, '/content/label_encoder.joblib')
print("✅ LabelEncoder saved successfully!")


# Save the TF-IDF vectorizer
tfidf_path = "/content/tfidf.joblib"
joblib.dump(tfidf, tfidf_path)
print("✅ TF-IDF Vectorizer saved successfully!")


joblib.dump(model, '/content/Email_Subject_NLP.joblib')

!pip install streamlit
!pip install pyngrok

!streamlit run /content/app.py &> /content/logs.txt &

!npx localtunnel --port 8501

