# -*- coding: utf-8 -*-
"""News Category Classification NLP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kFSjZRD9w-EvwF0pFonAfCwxh04G4JeB

# **News Category Classification**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/bbc-text.csv.zip')
df.head()

df = df.rename(columns={"text":"News_Headline"})
df.head()

df["category"].value_counts()

df["category"].unique()

"""## **Balanceng (RandomOverSampling)**"""

from imblearn.over_sampling import RandomOverSampler

# Assuming df is your dataframe with the "category" column
X = df.drop("category", axis=1)  # features
y = df["category"]  # target

# Initialize the RandomOverSampler
ros = RandomOverSampler(random_state=42)

# Apply the oversampling
X_resampled, y_resampled = ros.fit_resample(X, y)

# Combine the resampled features and target back into a DataFrame
df_resampled = X_resampled.copy()
df_resampled["category"] = y_resampled

# Check the new distribution
print(df_resampled["category"].value_counts())

df = df_resampled

df["category"].value_counts()

sns.countplot(data=df, x="category", palette="Set1", orient='h',legend=False)
plt.show()

a = df['category'].unique()
plt.pie(df["category"].value_counts(),labels=a,autopct="%.2f%%")
plt.show()

"""# **Text Preprocessing**"""

import nltk
# Download the 'punkt_tab' data package
nltk.download('punkt_tab')
import string
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords

from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

lemmatizer = WordNetLemmatizer()

# Lower case
# Tokenize
# Removing Special characters
# Removing stop words and punctuation
# Lemmatization

def transform(text):
    text=text.lower()
    text=nltk.word_tokenize(text)

    y=[]
    for i in text:
         if i.isalnum(): #isalnum mean alphanumeric or numeric
            y.append(i)

    text=y[:]
    y.clear()
    for i in text:
        if i not in string.punctuation and i not in stopwords.words('english'):
            y.append(i)

    text = y[:]
    y.clear()
    for i in text:
        y.append(lemmatizer.lemmatize(i))

    return " ".join(y)

df["Transformed_Text"]=df["News_Headline"].apply(transform)

df.head()

"""## **Label Encoding and TFIDF**"""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['category_encoded'] = le.fit_transform(df['category'])

df['category_encoded'].unique()

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(
    ngram_range=(1, 3),        # unigrams and bigrams
    max_df=0.8,                # ignore very common words
    min_df=2,                  # ignore very rare words
    max_features=10000,         # limit number of features
    sublinear_tf=True,         # log-scaling on term frequency
    use_idf=True,              # enable IDF
    smooth_idf=True            # add 1 to DF for stability
)

x = tfidf.fit_transform(df["Transformed_Text"]).toarray()

y=df["category"].values
y

"""# **Test Train Split**"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=50)

"""# **Model Building**"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

model = MultinomialNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred)*100)

model.score(X_train, y_train)*100

df["Transformed_Text"][1] #buss

df["Transformed_Text"][2] #sports

"""# **Streamlit App**"""

import joblib

# Save LabelEncoder
joblib.dump(le, '/content/label_encoder.joblib')
print("✅ LabelEncoder saved successfully!")


# Save the TF-IDF vectorizer
tfidf_path = "/content/tfidf.joblib"
joblib.dump(tfidf, tfidf_path)
print("✅ TF-IDF Vectorizer saved successfully!")


joblib.dump(model, '/content/Email_Subject_NLP.joblib')

!pip install streamlit
!pip install pyngrok

!streamlit run /content/app.py &> /content/logs.txt &

!npx localtunnel --port 8502

