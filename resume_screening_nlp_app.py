# -*- coding: utf-8 -*-
"""Resume Screening NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16GrC0AazB-0r9md4r0DIec233j5vL4yL
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
df=pd.read_csv(r'/content/UpdatedResumeDataSet.csv.zip')

df.head()

df["Resume"][0]

df.sample(10)

print(len(df["Category"].unique()))

import warnings
warnings.simplefilter("ignore") #these two lines are just to remove written text that is shown above graph

plt.figure(figsize=(10,3))
sns.countplot(x=df["Category"],palette="husl")
plt.xticks(rotation=90)
plt.show()

"""**Data Cleaning**"""

import re
def cleanResume(txt):
    txt = re.sub('http\S+\s?', ' ', txt)  # remove URLs
    txt = re.sub('RT|cc', ' ', txt)  # remove RT and cc
    txt = re.sub('#\S+', '', txt)  # remove hashtags
    txt = re.sub('@\S+', '  ', txt)  # remove mentions
    txt = re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), ' ', txt)  # remove punctuations
    txt = re.sub(r'[^\x00-\x7f]',r' ', txt)
    txt = re.sub('\s+', ' ', txt)
    return txt

df['Resume'][0]

cleanResume(df['Resume'][0])

df['Resume']=df['Resume'].apply(cleanResume)
df['Resume'][0]

from sklearn.preprocessing import LabelEncoder
lb=LabelEncoder()
df['Category']=lb.fit_transform(df['Category'])
df['Category']

from sklearn.feature_extraction.text import TfidfVectorizer
tfidfv=TfidfVectorizer(stop_words="english")
tfidfv.fit(df['Resume'])
converted_text=tfidfv.transform(df['Resume'])
converted_text

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(converted_text,df['Category'],test_size=0.2,random_state=42)

x_train.shape,x_test.shape,y_train.shape,y_test.shape

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score,confusion_matrix
knn=KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train,y_train)
y_pred=knn.predict(x_test)
accuracy_score(y_pred,y_test)

import pickle
pickle.dump(knn,open('model.pkl','wb'))
pickle.dump(tfidfv,open('tfidfv.pkl','wb'))

my_input=df["Resume"][500]

import pickle
knn=pickle.load(open('model.pkl','rb'))
cleaned_resume=cleanResume(my_input)
input_features=tfidfv.transform([cleaned_resume])
prediction=knn.predict(input_features)

category_mapping={
    6:"Data Science",
    12:"HR",
    0:"Advocate",
    1:"Arts",
    24:"Web Designing",
    16:"Mechanical Engineer",
    22:"Sales",
    14:"Health and fitness",
    5:"Civil Engineer",
    15:"Java Developer",
    4:"Business Analyst",
    21:"SAP Developer",
    2:"Automation Testing",
    11:"Electrical Engineering",
    18:"Operations Manager",
    20:"Python Developer",
    8:"Python Developer",
    17:"Network Security Engineer",
    19:"PMO",
    7:"Database",
    13:"Hadoop",
    10:"ETL Developer",
    9:"DotNet Developer",
    3:"Blockchain",
    23:"Testing"
}
category_Name=category_mapping[prediction[0]]
print(category_Name)

print(prediction)

"""***Saving Model***"""

import joblib

# Save the TF-IDF vectorizer
tfidf_path = "/content/tfidfv.joblib"
joblib.dump(tfidfv, tfidf_path)
print("âœ… TF-IDF Vectorizer saved successfully!")


joblib.dump(knn, '/content/Resume_Screening_NLP.joblib')

"""**Streamlit App Making**"""

!pip install streamlit
!pip install pyngrok

!pip install PyPDF2 pytesseract joblib

# now go to content at side and right click on content and click "new file" and name new file as "app.py"

!npm install localtunnel

!streamlit run /content/app.py &> /content/logs.txt &

!npx localtunnel --port 8501

